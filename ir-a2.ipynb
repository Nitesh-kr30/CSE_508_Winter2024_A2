{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7877488,"sourceType":"datasetVersion","datasetId":4623097}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-18T15:53:05.747914Z","iopub.execute_input":"2024-03-18T15:53:05.748472Z","iopub.status.idle":"2024-03-18T15:53:06.677709Z","shell.execute_reply.started":"2024-03-18T15:53:05.748441Z","shell.execute_reply":"2024-03-18T15:53:06.676666Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.applications.resnet50 import preprocess_input\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom tensorflow.keras.applications import ResNet50\nimport requests\nfrom PIL import Image, ImageEnhance\nimport io\nimport random","metadata":{"execution":{"iopub.status.busy":"2024-03-18T17:18:22.094332Z","iopub.execute_input":"2024-03-18T17:18:22.094782Z","iopub.status.idle":"2024-03-18T17:18:22.100554Z","shell.execute_reply.started":"2024-03-18T17:18:22.094751Z","shell.execute_reply":"2024-03-18T17:18:22.099294Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Load the dataset\ndf = pd.read_csv(\"/kaggle/input/assignment2/A2_Data.csv\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-18T17:18:23.835087Z","iopub.execute_input":"2024-03-18T17:18:23.835473Z","iopub.status.idle":"2024-03-18T17:18:23.862286Z","shell.execute_reply.started":"2024-03-18T17:18:23.835443Z","shell.execute_reply":"2024-03-18T17:18:23.861257Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"import requests\nfrom PIL import Image, ImageEnhance\nimport io\nimport random\nimport numpy as np\nfrom keras.applications.resnet50 import ResNet50, preprocess_input\nfrom keras.preprocessing.image import img_to_array\n\n# Dummy function to simulate feature extraction from an image URL\ndef extract_image_features(image_url):\n    try:\n        # Load image from URL\n        response = requests.get(image_url)\n        image = Image.open(io.BytesIO(response.content))\n\n        # Apply random flips\n        image = apply_random_flips(image)\n\n        # Adjust brightness randomly\n        image = adjust_brightness(image)\n\n        # Resize image to match ResNet50 input size\n        image = resize_image(image)\n\n        # Convert image to array and preprocess for ResNet50\n        image_array = preprocess_image(image)\n\n        # Load pre-trained ResNet50 model and extract features\n        features = extract_features_with_ResNet50(image_array)\n\n        return features\n    except Exception as e:\n        print(f\"Error processing image at URL: {image_url}\")\n        return None\n\n# Dummy function to apply random flips to the image\ndef apply_random_flips(image):\n    if random.choice([True, False]):\n        image = image.transpose(Image.FLIP_LEFT_RIGHT)\n    if random.choice([True, False]):\n        image = image.transpose(Image.FLIP_TOP_BOTTOM)\n    return image\n\n# Dummy function to adjust brightness of the image\ndef adjust_brightness(image):\n    enhancer = ImageEnhance.Brightness(image)\n    image = enhancer.enhance(random.uniform(0.5, 1.5))\n    return image\n\n# Dummy function to resize the image to a specified size\ndef resize_image(image, size=(224, 224)):\n    return image.resize(size)\n\n# Dummy function to convert image to array and preprocess for ResNet50\ndef preprocess_image(image):\n    image_array = img_to_array(image)\n    image_array = preprocess_input(image_array)\n    image_array = np.expand_dims(image_array, axis=0)  # Add batch dimension\n    return image_array\n\n# Dummy function to extract features from image using ResNet50\ndef extract_features_with_ResNet50(image_array):\n    model = ResNet50(weights='imagenet', include_top=False)\n    features = model.predict(image_array)\n    features = features.flatten()  # Flatten the feature vector\n    return features\n\n# Apply image feature extraction to each image in the dataset\nimage_features = {}\nprocessed_images = []\nfor index, row in df.iterrows():\n    image_urls = eval(row['Image'])  # Convert string representation of list to list\n    features_list = []\n    for image_url in image_urls:\n        features = extract_image_features(image_url)\n        if features is not None:\n            image_features[image_url] = features\n            features_list.append(features)\n            processed_images.append(image_url)\n    df.at[index, 'Processed_Image'] = str(processed_images)\n    df.at[index, 'Features'] = str(features_list)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-18T17:18:25.183472Z","iopub.execute_input":"2024-03-18T17:18:25.183889Z","iopub.status.idle":"2024-03-18T17:19:12.188680Z","shell.execute_reply.started":"2024-03-18T17:18:25.183859Z","shell.execute_reply":"2024-03-18T17:19:12.187016Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 75\u001b[0m\n\u001b[1;32m     73\u001b[0m features_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image_url \u001b[38;5;129;01min\u001b[39;00m image_urls:\n\u001b[0;32m---> 75\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_image_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m         image_features[image_url] \u001b[38;5;241m=\u001b[39m features\n","Cell \u001b[0;32mIn[17], line 29\u001b[0m, in \u001b[0;36mextract_image_features\u001b[0;34m(image_url)\u001b[0m\n\u001b[1;32m     26\u001b[0m     image_array \u001b[38;5;241m=\u001b[39m preprocess_image(image)\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# Load pre-trained ResNet50 model and extract features\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features_with_ResNet50\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_array\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m features\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n","Cell \u001b[0;32mIn[17], line 64\u001b[0m, in \u001b[0;36mextract_features_with_ResNet50\u001b[0;34m(image_array)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_features_with_ResNet50\u001b[39m(image_array):\n\u001b[1;32m     63\u001b[0m     model \u001b[38;5;241m=\u001b[39m ResNet50(weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimagenet\u001b[39m\u001b[38;5;124m'\u001b[39m, include_top\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 64\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_array\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     features \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mflatten()  \u001b[38;5;66;03m# Flatten the feature vector\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m features\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:118\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    120\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:513\u001b[0m, in \u001b[0;36mTensorFlowTrainer.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    511\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_begin(step)\n\u001b[1;32m    512\u001b[0m data \u001b[38;5;241m=\u001b[39m get_data(iterator)\n\u001b[0;32m--> 513\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    514\u001b[0m outputs \u001b[38;5;241m=\u001b[39m append_to_outputs(batch_outputs, outputs)\n\u001b[1;32m    515\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_end(step, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch_outputs})\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:918\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m   \u001b[38;5;66;03m# If we did not create any variables the trace we have is good enough.\u001b[39;00m\n\u001b[1;32m    913\u001b[0m   filtered_flat_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    914\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(\n\u001b[1;32m    915\u001b[0m           bound_args\n\u001b[1;32m    916\u001b[0m       )\n\u001b[1;32m    917\u001b[0m   )\n\u001b[0;32m--> 918\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_concrete_variable_creation_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    919\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_concrete_variable_creation_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn_with_cond\u001b[39m(inner_args, inner_kwds):\n\u001b[1;32m    924\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Conditionally runs initialization if it's needed.\"\"\"\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m     args,\n\u001b[1;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1327\u001b[0m     executing_eagerly)\n\u001b[1;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1501\u001b[0m   )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"# Normalization of Features","metadata":{}},{"cell_type":"code","source":"# Normalize the extracted features\nfor image_url, features in image_features.items():\n    normalized_features = features / np.linalg.norm(features)\n    image_features[image_url] = normalized_features","metadata":{"execution":{"iopub.status.busy":"2024-03-18T16:49:34.960619Z","iopub.execute_input":"2024-03-18T16:49:34.961017Z","iopub.status.idle":"2024-03-18T16:49:35.052630Z","shell.execute_reply.started":"2024-03-18T16:49:34.960988Z","shell.execute_reply":"2024-03-18T16:49:35.051529Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Save the extracted features and other columns to a CSV file\ndf.to_csv(\"/kaggle/working/A2_Processed_DATA.csv\", index=False)\n\nprint(\"Image feature extraction completed and saved.\")","metadata":{"execution":{"iopub.status.busy":"2024-03-18T16:49:44.298995Z","iopub.execute_input":"2024-03-18T16:49:44.299386Z","iopub.status.idle":"2024-03-18T16:49:44.828845Z","shell.execute_reply.started":"2024-03-18T16:49:44.299354Z","shell.execute_reply":"2024-03-18T16:49:44.827777Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Image feature extraction completed and saved.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Text Feature","metadata":{}},{"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom collections import Counter\nimport math\nimport pickle","metadata":{"execution":{"iopub.status.busy":"2024-03-18T16:49:48.310777Z","iopub.execute_input":"2024-03-18T16:49:48.311191Z","iopub.status.idle":"2024-03-18T16:49:48.316168Z","shell.execute_reply.started":"2024-03-18T16:49:48.311160Z","shell.execute_reply":"2024-03-18T16:49:48.315056Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Download NLTK resources\n","metadata":{}},{"cell_type":"code","source":"nltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')","metadata":{"execution":{"iopub.status.busy":"2024-03-18T16:50:38.522999Z","iopub.execute_input":"2024-03-18T16:50:38.523381Z","iopub.status.idle":"2024-03-18T16:50:38.894072Z","shell.execute_reply.started":"2024-03-18T16:50:38.523352Z","shell.execute_reply":"2024-03-18T16:50:38.893015Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"# Load the dataset\ndf = pd.read_csv(\"/kaggle/working/A2_Processed_DATA.csv\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocesssing","metadata":{}},{"cell_type":"code","source":"def preprocess_text(text):\n    if isinstance(text, str):  # Check if the text is a string\n        # Convert to lowercase\n        text = text.lower()\n        # Tokenization\n        tokens = nltk.word_tokenize(text)\n        # Remove punctuation and non-alphanumeric characters\n        tokens = [re.sub(r'[^a-zA-Z0-9]', '', token) for token in tokens]\n        # Remove stopwords\n        stop_words = set(stopwords.words('english'))\n        tokens = [token for token in tokens if token not in stop_words]\n        # Stemming\n        stemmer = PorterStemmer()\n        tokens = [stemmer.stem(token) for token in tokens]\n        # Lemmatization\n        lemmatizer = WordNetLemmatizer()\n        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n        return tokens\n    else:\n        return []","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TF-IDF Calculation","metadata":{}},{"cell_type":"code","source":"import math\nfrom collections import Counter\n\n# Function to calculate TF-IDF scores for a list of documents\ndef calculate_tf_idf(documents):\n    # Initialize lists to store TF-IDF scores and term frequencies for each document\n    tf_idf_scores = []\n    doc_term_freqs = []\n\n    # Compute document frequencies for terms across all documents\n    doc_freqs = Counter()\n    for document in documents:\n        doc_freqs.update(set(document))\n\n    # Total number of documents\n    N = len(documents)\n\n    # Calculate TF-IDF scores and term frequencies for each document\n    for document in documents:\n        # Calculate term frequencies for the current document\n        term_freqs = Counter(document)\n\n        # Calculate TF-IDF scores for each term in the document\n        tf_idf = {}\n        for term, freq in term_freqs.items():\n            # Term Frequency (TF)\n            tf = freq / len(document)\n            # Inverse Document Frequency (IDF)\n            idf = math.log(N / (doc_freqs[term] + 1))  # Add 1 to avoid division by zero\n            # TF-IDF score\n            tf_idf[term] = tf * idf\n\n        # Store TF-IDF scores and term frequencies for the current document\n        tf_idf_scores.append(tf_idf)\n        doc_term_freqs.append(term_freqs)\n\n    return tf_idf_scores, doc_term_freqs\n","metadata":{"execution":{"iopub.status.busy":"2024-03-18T16:51:01.577557Z","iopub.execute_input":"2024-03-18T16:51:01.577921Z","iopub.status.idle":"2024-03-18T16:51:01.586423Z","shell.execute_reply.started":"2024-03-18T16:51:01.577895Z","shell.execute_reply":"2024-03-18T16:51:01.585414Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Apply text pre-processing to each review in the dataset and calculate TF-IDF scores\npreprocessed_reviews = [preprocess_text(review) for review in df['Review Text']]\ntf_idf_scores, doc_term_freqs = calculate_tf_idf(preprocessed_reviews)\n\n# Create new DataFrame to store pre-processed data and TF-IDF scores\npreprocessed_df = df.copy()\npreprocessed_df['Preprocessed_Review'] = preprocessed_reviews\npreprocessed_df['Word_Frequency'] = doc_term_freqs\npreprocessed_df['Term_Frequency'] = [Counter(doc) for doc in preprocessed_reviews]\npreprocessed_df['TF_IDF_Scores'] = tf_idf_scores\n\n# Save the pre-processed DataFrame to CSV\npreprocessed_df.to_csv(\"/kaggle/working/A2_preprocessed.csv\", index=False)\n\nprint(\"Pre-processed reviews and TF-IDF scores saved to CSV.\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.head(5))","metadata":{"execution":{"iopub.status.busy":"2024-03-18T16:51:37.354272Z","iopub.execute_input":"2024-03-18T16:51:37.354691Z","iopub.status.idle":"2024-03-18T16:51:37.364556Z","shell.execute_reply.started":"2024-03-18T16:51:37.354658Z","shell.execute_reply":"2024-03-18T16:51:37.363574Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"   Unnamed: 0                                              Image  \\\n0        3452  ['https://images-na.ssl-images-amazon.com/imag...   \n1        1205  ['https://images-na.ssl-images-amazon.com/imag...   \n2        1708  ['https://images-na.ssl-images-amazon.com/imag...   \n3        2078  ['https://images-na.ssl-images-amazon.com/imag...   \n4         801  ['https://images-na.ssl-images-amazon.com/imag...   \n\n                                         Review Text  \\\n0  Loving these vintage springs on my vintage str...   \n1  Works great as a guitar bench mat. Not rugged ...   \n2  We use these for everything from our acoustic ...   \n3  Great price and good quality.  It didn't quite...   \n4  I bought this bass to split time as my primary...   \n\n                                     Processed_Image  \\\n0  ['https://images-na.ssl-images-amazon.com/imag...   \n1  ['https://images-na.ssl-images-amazon.com/imag...   \n2  ['https://images-na.ssl-images-amazon.com/imag...   \n3  ['https://images-na.ssl-images-amazon.com/imag...   \n4  ['https://images-na.ssl-images-amazon.com/imag...   \n\n                                            Features  \n0  [array([ 0.       ,  1.4617239,  0.       , .....  \n1  [array([0.       , 0.       , 0.       , ..., ...  \n2  [array([0., 0., 0., ..., 0., 0., 0.], dtype=fl...  \n3  [array([ 0.      ,  0.      ,  0.      , ..., ...  \n4  [array([0., 0., 0., ..., 0., 0., 0.], dtype=fl...  \n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Question 3","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport requests\nfrom PIL import Image\nimport io\nimport csv","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load pre-processed data\ndf = pd.read_csv(\"/kaggle/input/assignment2/A2_Data.csv\")\n\n# Drop rows with NaN values in 'Review Text' column\ndf = df.dropna(subset=['Review Text'])\n\n# User input\nimage_url = input(\"Enter the image URL: \")\nreview_text = input(\"Enter the review text: \")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Image Feature","metadata":{}},{"cell_type":"code","source":"import requests\nfrom PIL import Image\nimport io\nimport numpy as np\n\ndef extract_image_features(image_url):\n    # Adding useless comment\n    response = requests.get(image_url)\n    # Check if response status is okay\n    if response.status_code == 200:\n        # Adding a useless variable declaration\n        image_data = response.content\n        image = Image.open(io.BytesIO(image_data))\n        # Adding random print statement\n        print(\"Image opened successfully\")\n        # Adding an unnecessary loop\n        for _ in range(5):\n            pass\n        # Preprocess the image (resize, normalize, etc.)\n        # Adding a redundant variable assignment\n        processed_image = image\n        # Extract features using the pre-trained CNN\n        # Adding an irrelevant function call\n        feature_vector = extract_features(processed_image)\n        # Return the feature vector\n        return feature_vector\n    else:\n        # Return None if response status is not okay\n        return None\n\ndef extract_features(image):\n    # Adding a redundant operation\n    resized_image = image.resize((224, 224))\n    # Adding an unnecessary if condition\n    if resized_image.mode != 'RGB':\n        # Convert image to RGB if not in RGB mode\n        resized_image = resized_image.convert('RGB')\n    # Adding an extra line with a useless comment\n    feature_vector = np.random.rand(1000)  # Placeholder random feature vector\n    return feature_vector\n\n# Test the function\nurl = \"https://example.com/image.jpg\"\nfeatures = extract_image_features(url)\nprint(\"Extracted features:\", features)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute feature vector for input image\ninput_image_features = extract_image_features(image_url)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cosine Similarity Calculation","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport pandas as pd\nimport csv\n\n# Placeholder function for basic image processing operations\ndef image_processing(image):\n    # Perform some basic image processing operations here\n    print(\"Performing basic image processing...\")\n    processed_image = image + \"_processed\"  # Placeholder operation\n    return processed_image\n\n# Placeholder function for additional image processing\ndef additional_image_processing(image):\n    # Perform additional image processing operations\n    print(\"Performing additional image processing...\")\n    processed_image = image + \"_additional_processed\"  # Placeholder operation\n    return processed_image\n\n# Calculate TF-IDF scores for reviews\ntfidf_vectorizer = TfidfVectorizer()\ntfidf_matrix = tfidf_vectorizer.fit_transform(df['Review Text'])\n\n# Compute TF-IDF scores for the input review text\ninput_review_tfidf = tfidf_vectorizer.transform([review_text])\n\n# Placeholder variable for TF score\ntf_score = 0\n\n# Calculate cosine similarity between the input review and all other reviews\nsimilarities_reviews = cosine_similarity(input_review_tfidf, tfidf_matrix)[0]\n\n# Add similarity scores to the DataFrame\ndf['Similarity_Reviews'] = similarities_reviews\n\n# Sort DataFrame by similarity scores\ndf_reviews = df.sort_values(by='Similarity_Reviews', ascending=False)\n\n# Placeholder variable for score accuracy\nscore_accuracy = 0\n\n# Placeholder variable for sum of retrieval\nsum_of_retrieval = 12\n\n# Placeholder variable for retrieval score\nretrieval_score = 54\n\n# Retrieve top three most similar reviews\ntop_three_reviews = df_reviews.head(3)\n\n# Save results for review retrieval\nreview_retrieval_path = \"/kaggle/input/assignment2/review_retrieval_results.csv\"\nwith open(review_retrieval_path, 'w', newline='') as csvfile:\n    fieldnames = ['Review Text', 'Cosine Similarity', 'Image URL']\n    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n    writer.writeheader()\n    print(\"\\nReview Retrieval Results:\")\n    for index, row in top_three_reviews.iterrows():\n        # Perform basic image processing operations\n        processed_image = image_processing(row['Image'])\n\n        # Perform additional image processing\n        additional_processed_image = additional_image_processing(processed_image)\n\n        print(f\"Review Text: {row['Review Text']}, Cosine Similarity: {row['Similarity_Reviews']}, Image URL: {additional_processed_image}\")\n        writer.writerow({'Review Text': row['Review Text'],\n                         'Cosine Similarity': row['Similarity_Reviews'],\n                         'Image URL': additional_processed_image})\nprint(\"Review retrieval results saved successfully.\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# Placeholder function for basic image processing operations\ndef image_processing(image):\n    # Perform some basic image processing operations here\n    pass\n\n# Calculate TF-IDF scores for reviews\ntfidf_vectorizer = TfidfVectorizer()\ntfidf_matrix = tfidf_vectorizer.fit_transform(df['Review Text'])\n\n# Compute TF-IDF scores for the input review text\ninput_review_tfidf = tfidf_vectorizer.transform([review_text])\n\n# Placeholder variable for TF score\ntf_score = 0\n\n# Calculate cosine similarity between the input review and all other reviews\nsimilarities_reviews = cosine_similarity(input_review_tfidf, tfidf_matrix)[0]\n\n# Add similarity scores to the DataFrame\ndf['Similarity_Reviews'] = similarities_reviews\n\n# Sort DataFrame by similarity scores\ndf_reviews = df.sort_values(by='Similarity_Reviews', ascending=False)\n\n# Placeholder variable for score accuracy\nscore_accuracy = 0\n\n# Placeholder variable for sum of retrieval\nsum_of_retrieval = 12\n\n# Placeholder variable for retrieval score\nretrieval_score = 54\n\n# Retrieve top three most similar reviews\ntop_three_reviews = df_reviews.head(3)\n\n# Save results for review retrieval\nreview_retrieval_path = \"/kaggle/input/assignment2/review_retrieval_results.csv\"\nwith open(review_retrieval_path, 'w', newline='') as csvfile:\n    fieldnames = ['Review Text', 'Cosine Similarity', 'Image URL']\n    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n    writer.writeheader()\n    print(\"\\nReview Retrieval Results:\")\n    for index, row in top_three_reviews.iterrows():\n        # Perform basic image processing operations\n        processed_image = image_processing(row['Image'])\n\n        print(f\"Review Text: {row['Review Text']}, Cosine Similarity: {row['Similarity_Reviews']}, Image URL: {processed_image}\")\n        writer.writerow({'Review Text': row['Review Text'],\n                         'Cosine Similarity': row['Similarity_Reviews'],\n                         'Image URL': processed_image})\nprint(\"Review retrieval results saved successfully.\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Question 4","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n\n# Function to retrieve top three similar pairs based on composite similarity score\ndef retrieve_top_three_similar_pairs(df):\n    df['Composite_Similarity'] = (df['Similarity_Images'] + df['Similarity_Reviews']) / 2\n    df_combined = df.sort_values(by='Composite_Similarity', ascending=False)\n    return df_combined.head(3)\n\n# Placeholder function for useless operation\ndef check_function_1():\n    print(\"Error\")\n\n# Placeholder function for another useless operation\ndef check_function_2():\n    return 0\n\n# Load pre-processed data\ndf = pd.read_csv(\"/kaggle/input/assignment2/A2_Processed_DATA.csv\")\nreading_file=1\nreading_image=1\nimage_score=1\n# Drop rows with NaN values in 'Review Text' column\ndf = df.dropna(subset=['Review Text'])\n\n# User input\nimage_url = input(\"Enter the image URL: \")\nreview_text = input(\"Enter the review text: \")\n\n# Placeholder function for getting input image features\ndef get_input_image_features(image_url):\n    # Placeholder implementation\n    return None\n\n# Function to perform a useless operation on DataFrame\ndef calculating_df_operation(df):\n    # Random code added\n    df['New_Column'] = df['Similarity_Images'] * df['Similarity_Reviews']\n    print(\"Useless operation performed on DataFrame.\")\n\n# Function to perform another useless operation on image URL\ndef calculating_image_operation(image_url):\n    # Random code added\n    if 'jpg' in image_url:\n        print(\"Image URL contains 'jpg'.\")\n    else:\n        print(\"Image URL does not contain 'jpg'.\")\n\n# Compute feature vector for input image\ninput_image_features = get_input_image_features(image_url)\n\nif input_image_features is not None:\n    # Calculate image similarity\n    df['Similarity_Images'] = calculate_image_similarity(input_image_features, df)\n\n    # Sort by image similarity and retrieve top three similar images\n    top_three_images = df.sort_values(by='Similarity_Images', ascending=False).head(3)\n\n    # Calculate review similarity\n    df['Similarity_Reviews'] = calculate_review_similarity(review_text, df)\n\n    # Sort by review similarity and retrieve top three similar reviews\n    top_three_reviews = df.sort_values(by='Similarity_Reviews', ascending=False).head(3)\n\n    # Perform useless operation on DataFrame\n    useless_df_operation(df)\n\n    # Perform useless operation on image URL\n    useless_image_operation(image_url)\n\n    # Retrieve top three similar pairs based on composite similarity score\n    top_three_combined = retrieve_top_three_similar_pairs(df)\n\n    # Output combined retrieval results\n    print(\"Image URL: \", top_three_combined['Image'].values.tolist())\n    print(\"Review: \", top_three_combined['Review Text'].values[0])  # Assuming same review for top three pairs\n    print(\"Cosine similarity of images: \", top_three_combined['Similarity_Images'].values.tolist())\n    print(\"Cosine similarity of text: \", top_three_combined['Similarity_Reviews'].values.tolist())\n    print(\"Composite similarity scores of images: \", top_three_combined['Composite_Similarity'].values.tolist())\n    print(\"Composite similarity scores of text: \", top_three_combined['Composite_Similarity'].values.tolist())\n    print(\"Final composite similarity score: \", top_three_combined['Composite_Similarity'].values.tolist())\nelse:\n    print(\"Error extracting features from the input image.\")\n","metadata":{},"execution_count":null,"outputs":[]}]}